{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "V28"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XH7FUN9Klr1w",
        "outputId": "4858608e-9bb2-416b-aa6f-b54df635f93d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "KPU8em0Pi__Z"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import cv2\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from datetime import datetime\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report, roc_auc_score, confusion_matrix\n",
        "from sklearn.utils import class_weight\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential, load_model\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.applications import ResNet50\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.callbacks import TensorBoard, EarlyStopping, ModelCheckpoint\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Path to your image dataset\n",
        "#data_dir = '/content/drive/MyDrive/Hemant/Projects/aptos2019-blindness-detection/G1/G1_images'\n",
        "#labels_file = '/home/gcekcse/Documents/ML_Project_hk/aptos2019-blindness-detection/G1/G1.csv'\n",
        "data_dir = '/content/drive/MyDrive/Hemant/Projects/aptos2019-blindness-detection/G1/G1_images/'\n",
        "labels_file = '/content/drive/MyDrive/Hemant/Projects/aptos2019-blindness-detection/G1/G1.csv'\n",
        "\n",
        "\n",
        "# Load labels\n",
        "df = pd.read_csv(labels_file)\n",
        "\n"
      ],
      "metadata": {
        "id": "eCjrvyAOjmQE"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Count the number of samples for each class\n",
        "class_counts = df['diagnosis'].value_counts().reset_index()\n",
        "\n",
        "# Rename the columns for better understanding\n",
        "class_counts.columns = ['Class', 'Number of Samples']\n",
        "\n",
        "# Display the tabular view\n",
        "print(class_counts)"
      ],
      "metadata": {
        "id": "w--Fca-FjmNv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "59af6981-eba5-476e-ec2b-251156fc1e40"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   Class  Number of Samples\n",
            "0      0               1805\n",
            "1      2                999\n",
            "2      1                370\n",
            "3      4                295\n",
            "4      3                193\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Split data into training and testing\n",
        "train_df, test_df = train_test_split(df, test_size=0.2, random_state=42)\n",
        "\n",
        "# Display sample data\n",
        "print(train_df.head())"
      ],
      "metadata": {
        "id": "PBiuvy2ajmLW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "afba4d17-8f2b-44c0-818e-82207e1a9601"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "           id_code  diagnosis\n",
            "2181  996f9bba4ef0          0\n",
            "570   286e9981dd9b          0\n",
            "3389  eae70f527755          0\n",
            "343   19244004583f          3\n",
            "1801  7f0ffeb0a333          0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''import cv2\n",
        "import os\n",
        "\n",
        "image_directory = '/content/drive/MyDrive/Hemant/Projects/aptos2019-blindness-detection/G1/G1_images'\n",
        "image_files = os.listdir(image_directory)\n",
        "\n",
        "# Check for files\n",
        "if not image_files:\n",
        "    print(\"No files found in the specified directory.\")\n",
        "else:\n",
        "    print(f\"Found {len(image_files)} files in the directory.\")\n",
        "\n",
        "# Loop through each image file and attempt to load it\n",
        "for image_file in image_files:\n",
        "    image_path = os.path.join(image_directory, image_file)\n",
        "    try:\n",
        "        image = cv2.imread(image_path)\n",
        "        if image is None:\n",
        "            print(f\"Error: Unable to load image at {image_path}\")\n",
        "        else:\n",
        "            print(f\"Successfully loaded image at {image_path}\")\n",
        "    except Exception as e:\n",
        "        print(f\"Exception occurred for {image_path}: {e}\")\n",
        "\n",
        "# Optionally, you can keep a list of the problematic files\n",
        "problematic_files = []\n",
        "for image_file in image_files:\n",
        "    image_path = os.path.join(image_directory, image_file)\n",
        "    if cv2.imread(image_path) is None:\n",
        "        problematic_files.append(image_file)\n",
        "\n",
        "print(\"Problematic files that could not be loaded:\")\n",
        "for file in problematic_files:\n",
        "    print(file)\n",
        "'''"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 140
        },
        "collapsed": true,
        "id": "4BJ_YEAzl_M7",
        "outputId": "de4039b0-1909-4c2c-8631-9bbd090f9a8f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'import cv2\\nimport os\\n\\nimage_directory = \\'/content/drive/MyDrive/Hemant/Projects/aptos2019-blindness-detection/G1/G1_images\\'\\nimage_files = os.listdir(image_directory)\\n\\n# Check for files\\nif not image_files:\\n    print(\"No files found in the specified directory.\")\\nelse:\\n    print(f\"Found {len(image_files)} files in the directory.\")\\n\\n# Loop through each image file and attempt to load it\\nfor image_file in image_files:\\n    image_path = os.path.join(image_directory, image_file)\\n    try:\\n        image = cv2.imread(image_path)\\n        if image is None:\\n            print(f\"Error: Unable to load image at {image_path}\")\\n        else:\\n            print(f\"Successfully loaded image at {image_path}\")\\n    except Exception as e:\\n        print(f\"Exception occurred for {image_path}: {e}\")\\n\\n# Optionally, you can keep a list of the problematic files\\nproblematic_files = []\\nfor image_file in image_files:\\n    image_path = os.path.join(image_directory, image_file)\\n    if cv2.imread(image_path) is None:\\n        problematic_files.append(image_file)\\n\\nprint(\"Problematic files that could not be loaded:\")\\nfor file in problematic_files:\\n    print(file)\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import os\n",
        "import numpy as np\n",
        "\n",
        "# Directory containing the images\n",
        "image_directory = '/content/drive/MyDrive/Hemant/Projects/aptos2019-blindness-detection/G1/G1_images'\n",
        "data_dir = image_directory  # You can change this if needed\n",
        "IMG_SIZE = 224  # Resize images to 224x224\n",
        "\n",
        "# Function to check and load images\n",
        "def load_images(image_directory):\n",
        "    image_files = os.listdir(image_directory)\n",
        "\n",
        "    # Check for files\n",
        "    if not image_files:\n",
        "        print(\"No files found in the specified directory.\")\n",
        "        return [], []\n",
        "    else:\n",
        "        print(f\"Found {len(image_files)} files in the directory.\")\n",
        "\n",
        "    # Lists to store successfully loaded images and problematic files\n",
        "    loaded_images = []\n",
        "    problematic_files = []\n",
        "\n",
        "    # Loop through each image file and attempt to load it\n",
        "    for image_file in image_files:\n",
        "        image_path = os.path.join(image_directory, image_file)\n",
        "        try:\n",
        "            image = cv2.imread(image_path)\n",
        "            if image is None:\n",
        "                print(f\"Error: Unable to load image at {image_path}\")\n",
        "                problematic_files.append(image_file)\n",
        "            else:\n",
        "                print(f\"Successfully loaded image at {image_path}\")\n",
        "                loaded_images.append((image_file, image))  # Store filename and image\n",
        "        except Exception as e:\n",
        "            print(f\"Exception occurred for {image_path}: {e}\")\n",
        "            problematic_files.append(image_file)\n",
        "\n",
        "    print(\"Problematic files that could not be loaded:\")\n",
        "    for file in problematic_files:\n",
        "        print(file)\n",
        "\n",
        "    return loaded_images, problematic_files\n",
        "\n",
        "# Function to preprocess images\n",
        "def preprocess_image(image):\n",
        "    img = cv2.resize(image, (IMG_SIZE, IMG_SIZE))  # Resize the image\n",
        "    img = img / 255.0  # Normalize\n",
        "    return img\n",
        "\n",
        "def preprocess_data(dataframe, loaded_images):\n",
        "    X = []\n",
        "    y = []\n",
        "    file_dict = {file[0]: file[1] for file in loaded_images}  # Map filenames to images\n",
        "\n",
        "    for idx, row in dataframe.iterrows():\n",
        "        img_name = row['id_code'] + \".png\"  # Assuming .png extension\n",
        "        if img_name in file_dict:  # Check if the image was loaded successfully\n",
        "            img = preprocess_image(file_dict[img_name])  # Preprocess the image\n",
        "            X.append(img)\n",
        "            y.append(row['diagnosis'])  # Assuming 'diagnosis' contains the target label\n",
        "\n",
        "        if idx % 100 == 0:  # Log progress every 100 images\n",
        "            print(f\"Processed {idx + 1}/{len(dataframe)} images.\")\n",
        "\n",
        "    return np.array(X), np.array(y)\n",
        "\n",
        "# Load images\n",
        "print(\"Loading images from directory...\")\n",
        "loaded_images, problematic_files = load_images(image_directory)\n",
        "\n",
        "# Preprocess data\n",
        "print(\"Preprocessing training data...\")\n",
        "X_train, y_train = preprocess_data(train_df, loaded_images)\n",
        "\n",
        "print(\"Preprocessing test data...\")\n",
        "X_test, y_test = preprocess_data(test_df, loaded_images)\n",
        "\n",
        "print(\"Preprocessing completed.\")\n",
        "\n",
        "# Define the directory where you want to save the data\n",
        "save_dir = '/content/drive/MyDrive/Hemant/Projects/aptos2019-blindness-detection/data/'\n",
        "\n",
        "# Create the directory if it doesn't exist\n",
        "os.makedirs(save_dir, exist_ok=True)\n",
        "\n",
        "# Save training data\n",
        "np.save(os.path.join(save_dir, 'X_train.npy'), X_train)\n",
        "np.save(os.path.join(save_dir, 'y_train.npy'), y_train)\n",
        "\n",
        "# Save test data\n",
        "np.save(os.path.join(save_dir, 'X_test.npy'), X_test)\n",
        "np.save(os.path.join(save_dir, 'y_test.npy'), y_test)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 592
        },
        "collapsed": true,
        "id": "xVCdSUNjqY6h",
        "outputId": "b754ab9c-dc3a-4efb-c0a8-46ccbf90037e"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading images from directory...\n",
            "Found 1216 files in the directory.\n",
            "Successfully loaded image at /content/drive/MyDrive/Hemant/Projects/aptos2019-blindness-detection/G1/G1_images/357f02a779d7.png\n",
            "Successfully loaded image at /content/drive/MyDrive/Hemant/Projects/aptos2019-blindness-detection/G1/G1_images/365f8c01d994.png\n",
            "Successfully loaded image at /content/drive/MyDrive/Hemant/Projects/aptos2019-blindness-detection/G1/G1_images/35df2bc6ae95.png\n",
            "Successfully loaded image at /content/drive/MyDrive/Hemant/Projects/aptos2019-blindness-detection/G1/G1_images/359bab5d784b.png\n",
            "Successfully loaded image at /content/drive/MyDrive/Hemant/Projects/aptos2019-blindness-detection/G1/G1_images/35cd9832fc0a.png\n",
            "Successfully loaded image at /content/drive/MyDrive/Hemant/Projects/aptos2019-blindness-detection/G1/G1_images/356304d15a5c.png\n",
            "Successfully loaded image at /content/drive/MyDrive/Hemant/Projects/aptos2019-blindness-detection/G1/G1_images/35beb47fe159.png\n",
            "Successfully loaded image at /content/drive/MyDrive/Hemant/Projects/aptos2019-blindness-detection/G1/G1_images/37c4dfe03aba.png\n",
            "Successfully loaded image at /content/drive/MyDrive/Hemant/Projects/aptos2019-blindness-detection/G1/G1_images/367c7049929c.png\n",
            "Successfully loaded image at /content/drive/MyDrive/Hemant/Projects/aptos2019-blindness-detection/G1/G1_images/370f575adb23.png\n",
            "Successfully loaded image at /content/drive/MyDrive/Hemant/Projects/aptos2019-blindness-detection/G1/G1_images/3730c322d35b.png\n",
            "Successfully loaded image at /content/drive/MyDrive/Hemant/Projects/aptos2019-blindness-detection/G1/G1_images/369229040a34.png\n",
            "Successfully loaded image at /content/drive/MyDrive/Hemant/Projects/aptos2019-blindness-detection/G1/G1_images/36e4b704b905.png\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-6-ee1722aced03>\u001b[0m in \u001b[0;36m<cell line: 71>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     69\u001b[0m \u001b[0;31m# Load images\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Loading images from directory...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 71\u001b[0;31m \u001b[0mloaded_images\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mproblematic_files\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_images\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_directory\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     72\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m \u001b[0;31m# Preprocess data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-6-ee1722aced03>\u001b[0m in \u001b[0;36mload_images\u001b[0;34m(image_directory)\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0mimage_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_directory\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimage_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m             \u001b[0mimage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mimage\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Error: Unable to load image at {image_path}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import os\n",
        "\n",
        "# Define the directory where your data is stored\n",
        "data_dir = '/content/drive/MyDrive/Hemant/Projects/aptos2019-blindness-detection/data/'\n",
        "\n",
        "# Load data\n",
        "X_train = np.load(os.path.join(data_dir, 'X_train.npy'))\n",
        "y_train = np.load(os.path.join(data_dir, 'y_train.npy'))\n",
        "X_test = np.load(os.path.join(data_dir, 'X_test.npy'))\n",
        "y_test = np.load(os.path.join(data_dir, 'y_test.npy'))"
      ],
      "metadata": {
        "id": "FSmcBRGgjmGv"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "from tensorflow.keras.applications import ResNet50\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Flatten, Dense, Dropout\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "# Set the image size\n",
        "IMG_SIZE = 224  # Change this if your input size is different\n",
        "\n",
        "# Load ResNet50 base model (pre-trained on ImageNet)\n",
        "base_model = ResNet50(weights='imagenet', include_top=False, input_shape=(IMG_SIZE, IMG_SIZE, 3))\n",
        "\n",
        "# Freeze the base model\n",
        "base_model.trainable = False\n",
        "\n",
        "# Add custom layers on top\n",
        "model = Sequential([\n",
        "    base_model,\n",
        "    Flatten(),\n",
        "    Dense(512, activation='relu'),\n",
        "    Dropout(0.5),\n",
        "    Dense(5, activation='softmax')  # Assuming 5 DR severity levels\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer=Adam(learning_rate=1e-4), loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Build the model to define all shapes\n",
        "model.build((None, IMG_SIZE, IMG_SIZE, 3))  # Add this line to explicitly define the input shape\n",
        "\n",
        "# View the model summary\n",
        "model.summary()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nEz4rT-nyGnz",
        "outputId": "c938b162-83f5-42a0-d82d-be0df8be8dc8"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " resnet50 (Functional)       (None, 7, 7, 2048)        23587712  \n",
            "                                                                 \n",
            " flatten_1 (Flatten)         (None, 100352)            0         \n",
            "                                                                 \n",
            " dense_2 (Dense)             (None, 512)               51380736  \n",
            "                                                                 \n",
            " dropout_1 (Dropout)         (None, 512)               0         \n",
            "                                                                 \n",
            " dense_3 (Dense)             (None, 5)                 2565      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 74971013 (285.99 MB)\n",
            "Trainable params: 51383301 (196.01 MB)\n",
            "Non-trainable params: 23587712 (89.98 MB)\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Check if GPU is available\n",
        "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))\n",
        "\n",
        "# Data augmentation\n",
        "datagen = ImageDataGenerator(\n",
        "    rotation_range=20,\n",
        "    zoom_range=0.15,\n",
        "    width_shift_range=0.2,\n",
        "    height_shift_range=0.2,\n",
        "    shear_range=0.15,\n",
        "    horizontal_flip=True,\n",
        "    fill_mode=\"nearest\"\n",
        ")\n",
        "\n",
        "# Define the checkpoint callback to overwrite the saved model after each epoch\n",
        "checkpoint_callback = ModelCheckpoint(\n",
        "    filepath='/content/drive/MyDrive/Hemant/Projects/aptos2019-blindness-detection/models/model.keras',  # Change to .keras\n",
        "    save_best_only=False,\n",
        "    save_weights_only=False,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "# Early stopping callback\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)"
      ],
      "metadata": {
        "id": "XkxyoZODjmCh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dc687873-4d33-446b-b78c-4db18c3b573c"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Num GPUs Available:  0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Fit the model\n",
        "history = model.fit(\n",
        "    datagen.flow(X_train, y_train, batch_size=32),\n",
        "    validation_data=(X_test, y_test),\n",
        "    epochs=50,\n",
        "    callbacks=[checkpoint_callback, early_stopping]\n",
        ")\n"
      ],
      "metadata": {
        "id": "1watxedXjmAi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cae161f6-6310-4a4f-b74d-2138cffd9810"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n",
            "30/30 [==============================] - ETA: 0s - loss: 1.3765 - accuracy: 0.7954\n",
            "Epoch 1: saving model to /content/drive/MyDrive/Hemant/Projects/aptos2019-blindness-detection/models/model.keras\n",
            "30/30 [==============================] - 40s 1s/step - loss: 1.3765 - accuracy: 0.7954 - val_loss: 1.1100 - val_accuracy: 0.8157\n",
            "Epoch 2/50\n",
            "30/30 [==============================] - ETA: 0s - loss: 0.7088 - accuracy: 0.8395\n",
            "Epoch 2: saving model to /content/drive/MyDrive/Hemant/Projects/aptos2019-blindness-detection/models/model.keras\n",
            "30/30 [==============================] - 30s 996ms/step - loss: 0.7088 - accuracy: 0.8395 - val_loss: 0.7172 - val_accuracy: 0.8157\n",
            "Epoch 3/50\n",
            "30/30 [==============================] - ETA: 0s - loss: 0.6025 - accuracy: 0.8678\n",
            "Epoch 3: saving model to /content/drive/MyDrive/Hemant/Projects/aptos2019-blindness-detection/models/model.keras\n",
            "30/30 [==============================] - 30s 1s/step - loss: 0.6025 - accuracy: 0.8678 - val_loss: 0.7150 - val_accuracy: 0.8157\n",
            "Epoch 4/50\n",
            "30/30 [==============================] - ETA: 0s - loss: 0.5837 - accuracy: 0.8678\n",
            "Epoch 4: saving model to /content/drive/MyDrive/Hemant/Projects/aptos2019-blindness-detection/models/model.keras\n",
            "30/30 [==============================] - 29s 965ms/step - loss: 0.5837 - accuracy: 0.8678 - val_loss: 0.6922 - val_accuracy: 0.8157\n",
            "Epoch 5/50\n",
            "30/30 [==============================] - ETA: 0s - loss: 0.5821 - accuracy: 0.8667\n",
            "Epoch 5: saving model to /content/drive/MyDrive/Hemant/Projects/aptos2019-blindness-detection/models/model.keras\n",
            "30/30 [==============================] - 29s 965ms/step - loss: 0.5821 - accuracy: 0.8667 - val_loss: 0.7268 - val_accuracy: 0.8157\n",
            "Epoch 6/50\n",
            "30/30 [==============================] - ETA: 0s - loss: 0.6085 - accuracy: 0.8646\n",
            "Epoch 6: saving model to /content/drive/MyDrive/Hemant/Projects/aptos2019-blindness-detection/models/model.keras\n",
            "30/30 [==============================] - 29s 970ms/step - loss: 0.6085 - accuracy: 0.8646 - val_loss: 0.6979 - val_accuracy: 0.8157\n",
            "Epoch 7/50\n",
            "30/30 [==============================] - ETA: 0s - loss: 0.6075 - accuracy: 0.8657\n",
            "Epoch 7: saving model to /content/drive/MyDrive/Hemant/Projects/aptos2019-blindness-detection/models/model.keras\n",
            "30/30 [==============================] - 29s 977ms/step - loss: 0.6075 - accuracy: 0.8657 - val_loss: 0.6995 - val_accuracy: 0.8157\n",
            "Epoch 8/50\n",
            "30/30 [==============================] - ETA: 0s - loss: 0.5841 - accuracy: 0.8667\n",
            "Epoch 8: saving model to /content/drive/MyDrive/Hemant/Projects/aptos2019-blindness-detection/models/model.keras\n",
            "30/30 [==============================] - 29s 974ms/step - loss: 0.5841 - accuracy: 0.8667 - val_loss: 0.7380 - val_accuracy: 0.8157\n",
            "Epoch 9/50\n",
            "30/30 [==============================] - ETA: 0s - loss: 0.6037 - accuracy: 0.8657\n",
            "Epoch 9: saving model to /content/drive/MyDrive/Hemant/Projects/aptos2019-blindness-detection/models/model.keras\n",
            "30/30 [==============================] - 29s 975ms/step - loss: 0.6037 - accuracy: 0.8657 - val_loss: 0.6906 - val_accuracy: 0.8157\n",
            "Epoch 10/50\n",
            "30/30 [==============================] - ETA: 0s - loss: 0.5802 - accuracy: 0.8657\n",
            "Epoch 10: saving model to /content/drive/MyDrive/Hemant/Projects/aptos2019-blindness-detection/models/model.keras\n",
            "30/30 [==============================] - 29s 977ms/step - loss: 0.5802 - accuracy: 0.8657 - val_loss: 0.7409 - val_accuracy: 0.8157\n",
            "Epoch 11/50\n",
            "30/30 [==============================] - ETA: 0s - loss: 0.6104 - accuracy: 0.8667\n",
            "Epoch 11: saving model to /content/drive/MyDrive/Hemant/Projects/aptos2019-blindness-detection/models/model.keras\n",
            "30/30 [==============================] - 29s 975ms/step - loss: 0.6104 - accuracy: 0.8667 - val_loss: 0.6888 - val_accuracy: 0.8157\n",
            "Epoch 12/50\n",
            "30/30 [==============================] - ETA: 0s - loss: 0.5752 - accuracy: 0.8667\n",
            "Epoch 12: saving model to /content/drive/MyDrive/Hemant/Projects/aptos2019-blindness-detection/models/model.keras\n",
            "30/30 [==============================] - 30s 998ms/step - loss: 0.5752 - accuracy: 0.8667 - val_loss: 0.6955 - val_accuracy: 0.8157\n",
            "Epoch 13/50\n",
            "30/30 [==============================] - ETA: 0s - loss: 0.5796 - accuracy: 0.8667\n",
            "Epoch 13: saving model to /content/drive/MyDrive/Hemant/Projects/aptos2019-blindness-detection/models/model.keras\n",
            "30/30 [==============================] - 29s 979ms/step - loss: 0.5796 - accuracy: 0.8667 - val_loss: 0.6765 - val_accuracy: 0.8157\n",
            "Epoch 14/50\n",
            "30/30 [==============================] - ETA: 0s - loss: 0.5725 - accuracy: 0.8678\n",
            "Epoch 14: saving model to /content/drive/MyDrive/Hemant/Projects/aptos2019-blindness-detection/models/model.keras\n",
            "30/30 [==============================] - 29s 974ms/step - loss: 0.5725 - accuracy: 0.8678 - val_loss: 0.7127 - val_accuracy: 0.8157\n",
            "Epoch 15/50\n",
            "30/30 [==============================] - ETA: 0s - loss: 0.6121 - accuracy: 0.8667\n",
            "Epoch 15: saving model to /content/drive/MyDrive/Hemant/Projects/aptos2019-blindness-detection/models/model.keras\n",
            "30/30 [==============================] - 29s 970ms/step - loss: 0.6121 - accuracy: 0.8667 - val_loss: 0.7020 - val_accuracy: 0.8157\n",
            "Epoch 16/50\n",
            "30/30 [==============================] - ETA: 0s - loss: 0.5934 - accuracy: 0.8667\n",
            "Epoch 16: saving model to /content/drive/MyDrive/Hemant/Projects/aptos2019-blindness-detection/models/model.keras\n",
            "30/30 [==============================] - 29s 968ms/step - loss: 0.5934 - accuracy: 0.8667 - val_loss: 0.7697 - val_accuracy: 0.8157\n",
            "Epoch 17/50\n",
            "30/30 [==============================] - ETA: 0s - loss: 0.5674 - accuracy: 0.8678\n",
            "Epoch 17: saving model to /content/drive/MyDrive/Hemant/Projects/aptos2019-blindness-detection/models/model.keras\n",
            "30/30 [==============================] - 29s 973ms/step - loss: 0.5674 - accuracy: 0.8678 - val_loss: 0.6681 - val_accuracy: 0.8157\n",
            "Epoch 18/50\n",
            "30/30 [==============================] - ETA: 0s - loss: 0.5952 - accuracy: 0.8646\n",
            "Epoch 18: saving model to /content/drive/MyDrive/Hemant/Projects/aptos2019-blindness-detection/models/model.keras\n",
            "30/30 [==============================] - 29s 974ms/step - loss: 0.5952 - accuracy: 0.8646 - val_loss: 0.6762 - val_accuracy: 0.8157\n",
            "Epoch 19/50\n",
            "30/30 [==============================] - ETA: 0s - loss: 0.5706 - accuracy: 0.8678\n",
            "Epoch 19: saving model to /content/drive/MyDrive/Hemant/Projects/aptos2019-blindness-detection/models/model.keras\n",
            "30/30 [==============================] - 29s 981ms/step - loss: 0.5706 - accuracy: 0.8678 - val_loss: 0.7151 - val_accuracy: 0.8157\n",
            "Epoch 20/50\n",
            "30/30 [==============================] - ETA: 0s - loss: 0.5748 - accuracy: 0.8678\n",
            "Epoch 20: saving model to /content/drive/MyDrive/Hemant/Projects/aptos2019-blindness-detection/models/model.keras\n",
            "30/30 [==============================] - 29s 979ms/step - loss: 0.5748 - accuracy: 0.8678 - val_loss: 0.7074 - val_accuracy: 0.8157\n",
            "Epoch 21/50\n",
            "30/30 [==============================] - ETA: 0s - loss: 0.5778 - accuracy: 0.8678\n",
            "Epoch 21: saving model to /content/drive/MyDrive/Hemant/Projects/aptos2019-blindness-detection/models/model.keras\n",
            "30/30 [==============================] - 29s 967ms/step - loss: 0.5778 - accuracy: 0.8678 - val_loss: 0.6839 - val_accuracy: 0.8157\n",
            "Epoch 22/50\n",
            "17/30 [================>.............] - ETA: 9s - loss: 0.5561 - accuracy: 0.8768 "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import load_model\n",
        "from sklearn.metrics import classification_report, roc_auc_score\n",
        "\n",
        "# Step 1: Load the model\n",
        "print(\"Loading the model...\")\n",
        "#model = load_model(r'/content/drive/MyDrive/Hemant/Projects/aptos2019-blindness-detection/models/model.keras')\n",
        "\n",
        "# Define the directory where your data is stored\n",
        "data_dir = '/content/drive/MyDrive/Hemant/Projects/aptos2019-blindness-detection/data/'\n",
        "\n",
        "# Step 2: Load the test data\n",
        "print(\"Loading test data...\")\n",
        "X_test = np.load(os.path.join(data_dir, 'X_test.npy'))\n",
        "y_test = np.load(os.path.join(data_dir, 'y_test.npy'))\n",
        "\n",
        "print(f\"Test data shape: X_test: {X_test.shape}, y_test: {y_test.shape}\")\n",
        "\n",
        "# Step 3: Convert test data to tensor\n",
        "print(\"Converting X_test to tensor...\")\n",
        "X_test_tensor = tf.convert_to_tensor(X_test, dtype=tf.float32)\n",
        "\n",
        "# Step 4: Evaluate the model on the test set\n",
        "print(\"Evaluating the model on the test set...\")\n",
        "test_loss, test_acc = model.evaluate(X_test_tensor, y_test, verbose=1)\n",
        "print(f\"Test Loss: {test_loss:.4f}, Test Accuracy: {test_acc * 100:.2f}%\")\n",
        "\n",
        "# Step 5: Make predictions and print classification report\n",
        "print(\"Predicting on test data...\")\n",
        "y_pred = np.argmax(model.predict(X_test_tensor, verbose=1), axis=1)\n",
        "print(\"Generating classification report...\")\n",
        "print(classification_report(y_test, y_pred, zero_division=1))\n",
        "\n",
        "# Step 6: Compute ROC-AUC score\n",
        "print(\"Calculating predicted probabilities for ROC-AUC...\")\n",
        "y_pred_prob = model.predict(X_test_tensor, verbose=1)  # Get the predicted probabilities\n",
        "print(f\"Calculating AUC-ROC...\")\n",
        "print(f\"AUC-ROC: {roc_auc_score(y_test, y_pred_prob, multi_class='ovo')}\")\n"
      ],
      "metadata": {
        "id": "kbdDsKDzjl-q",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "376eb230-cc35-450f-985b-835f37188102"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading the model...\n",
            "Loading test data...\n",
            "Test data shape: X_test: (255, 224, 224, 3), y_test: (255,)\n",
            "Converting X_test to tensor...\n",
            "Evaluating the model on the test set...\n",
            "8/8 [==============================] - 6s 521ms/step - loss: 0.6822 - accuracy: 0.8157\n",
            "Test Loss: 0.6822, Test Accuracy: 81.57%\n",
            "Predicting on test data...\n",
            "8/8 [==============================] - 5s 492ms/step\n",
            "Generating classification report...\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.82      1.00      0.90       208\n",
            "           1       1.00      0.00      0.00         9\n",
            "           2       1.00      0.00      0.00        22\n",
            "           3       1.00      0.00      0.00         4\n",
            "           4       1.00      0.00      0.00        12\n",
            "\n",
            "    accuracy                           0.82       255\n",
            "   macro avg       0.96      0.20      0.18       255\n",
            "weighted avg       0.85      0.82      0.73       255\n",
            "\n",
            "Calculating predicted probabilities for ROC-AUC...\n",
            "8/8 [==============================] - 4s 524ms/step\n",
            "Calculating AUC-ROC...\n",
            "AUC-ROC: 0.6170891608391609\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Confusion Matrix\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "# Plotting the confusion matrix\n",
        "plt.figure(figsize=(10, 8))\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=np.unique(y_test), yticklabels=np.unique(y_test))\n",
        "plt.xlabel('Predicted Label')\n",
        "plt.ylabel('True Label')\n",
        "plt.title('Confusion Matrix')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "KGWb9_-jjl8W",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 718
        },
        "outputId": "3abcbdf2-1bb3-4330-cd01-fc767cfced39"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x800 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!tensorboard --logdir=logs/fit\n"
      ],
      "metadata": {
        "id": "vcT5yXuhjl6P",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5b211b1e-1e92-48e2-f628-80a9bb72bfdf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "NOTE: Using experimental fast data loading logic. To disable, pass\n",
            "    \"--load_fast=false\" and report issues on GitHub. More details:\n",
            "    https://github.com/tensorflow/tensorboard/issues/4784\n",
            "\n",
            "Serving TensorBoard on localhost; to expose to the network, use a proxy or pass --bind_all\n",
            "TensorBoard 2.15.2 at http://localhost:6006/ (Press CTRL+C to quit)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "S-FaARj7lUY7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "3qTlmtrvlUTD"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}